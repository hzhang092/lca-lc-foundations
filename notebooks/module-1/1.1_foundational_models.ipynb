{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "106cf38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e9c4a",
   "metadata": {},
   "source": [
    "## Initialising and invoking a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.chat_models import init_chat_model\n",
    "\n",
    "#model = init_chat_model(model=\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf3ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = model.invoke(\"What's the capital of the Moon?\")\n",
    "\n",
    "#response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d54271c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid NVIDIA_API_KEY already in environment. Delete to reset\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NVIDIA_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    nvapi_key = getpass.getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert nvapi_key.startswith(\n",
    "        \"nvapi-\"\n",
    "    ), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02333adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Core LC Chat Interface\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "model = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62a5ff0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\" The Moon does not have a capital. It's a natural satellite of Earth and lacks the necessary infrastructure and political structure to have a capital.\", additional_kwargs={}, response_metadata={'role': 'assistant', 'content': \" The Moon does not have a capital. It's a natural satellite of Earth and lacks the necessary infrastructure and political structure to have a capital.\", 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None, 'token_usage': {'prompt_tokens': 17, 'total_tokens': 48, 'completion_tokens': 31, 'prompt_tokens_details': None}, 'finish_reason': 'stop', 'model_name': 'mistralai/mixtral-8x7b-instruct-v0.1'}, id='lc_run--019c8648-2e0f-7d41-9d8f-750441fab063-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 17, 'output_tokens': 31, 'total_tokens': 48}, role='assistant')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = model.invoke(\"What's the capital of the Moon?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee8e957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Moon does not have a capital. It's a natural satellite of Earth and lacks the necessary infrastructure and political structure to have a capital.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "673a54e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'annotations': None,\n",
      " 'audio': None,\n",
      " 'content': \" The Moon does not have a capital. It's a natural satellite of \"\n",
      "            'Earth and lacks the necessary infrastructure and political '\n",
      "            'structure to have a capital.',\n",
      " 'finish_reason': 'stop',\n",
      " 'function_call': None,\n",
      " 'model_name': 'mistralai/mixtral-8x7b-instruct-v0.1',\n",
      " 'reasoning': None,\n",
      " 'reasoning_content': None,\n",
      " 'refusal': None,\n",
      " 'role': 'assistant',\n",
      " 'token_usage': {'completion_tokens': 31,\n",
      "                 'prompt_tokens': 17,\n",
      "                 'prompt_tokens_details': None,\n",
      "                 'total_tokens': 48},\n",
      " 'tool_calls': []}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d4e92a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Model(id='qwen/qwq-32b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mistralai/mistral-medium-3-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='institute-of-science-tokyo/llama-3.1-swallow-8b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='tokyotech-llm/llama-3-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='ibm/granite-guardian-3.0-8b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='igenius/italia_10b_instruct_16k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-4k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/nemotron-3-nano-30b-a3b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=True, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}}, thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}}, base_model=None),\n",
       " Model(id='nvidia/nvclip', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='google/gemma-3-4b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-vl-8b-v1', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='google/shieldgemma-9b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='ibm/granite-3.3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=True, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}}, thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}}, base_model=None),\n",
       " Model(id='mistralai/mathstral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-70b-reward', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='qwen/qwen3-235b-a22b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=True, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}}, thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}}, base_model=None),\n",
       " Model(id='speakleash/bielik-11b-v2.3-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-v3.1-terminus', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=True, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}}, thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}}, base_model=None),\n",
       " Model(id='gotocompany/gemma-2-9b-cpt-sahabatai-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='qwen/qwen3-next-80b-a3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='google/gemma-2-9b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-9b-it'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mistralai/mistral-small-3.1-24b-instruct-2503', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='rakuten/rakutenai-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-8k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='qwen/qwen2.5-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='moonshotai/kimi-k2-instruct-0905', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mistralai/magistral-small-2506', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='thudm/chatglm3-6b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='google/gemma-3n-e2b-it', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='tiiuae/falcon3-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/nemoretriever-parse', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mistralai/mistral-nemotron', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='zyphra/zamba2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='google/gemma-3-12b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mediatek/breeze-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-breeze-7b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='microsoft/phi-4-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='google/paligemma', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/paligemma', aliases=['ai-google-paligemma'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='google/gemma-3-1b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='yentinglin/llama-3-taiwan-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-128k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='microsoft/phi-3-vision-128k-instruct', model_type='nv-vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct', aliases=['ai-phi-3-vision-128k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='stepfun-ai/step-3.5-flash', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='bytedance/seed-oss-36b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-mini-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/nemotron-mini-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='meta/llama-3.2-1b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='google/gemma-3-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/nvidia-nemotron-nano-9b-v2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=True, thinking_prefix='/think', no_thinking_prefix='/no_think', thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-8b-v1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=True, thinking_prefix='detailed thinking on', no_thinking_prefix='detailed thinking off', thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='openai/gpt-oss-20b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mistralai/mistral-small-24b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='qwen/qwen3-next-80b-a3b-thinking', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='microsoft/phi-4-mini-flash-reasoning', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='01-ai/yi-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-yi-large'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='google/gemma-2-2b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='google/gemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-32b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='meta/llama-3.3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/usdcode-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-v3.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=True, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}}, thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}}, base_model=None),\n",
       " Model(id='meta/llama-4-scout-17b-16e-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/llama-3.3-nemotron-super-49b-v1.5', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=True, thinking_prefix='/think', no_thinking_prefix='/no_think', thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='baichuan-inc/baichuan2-13b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-nano-4b-v1.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=True, thinking_prefix='detailed thinking on', no_thinking_prefix='detailed thinking off', thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='microsoft/phi-3.5-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='igenius/colosseum_355b_instruct_16k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-128k-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='meta/llama-3.2-11b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='qwen/qwen2.5-coder-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='qwen/qwen2-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='aisingapore/sea-lion-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-sea-lion-7b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mistralai/mistral-large-3-675b-instruct-2512', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemotron-ultra-253b-v1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=True, thinking_prefix='detailed thinking on', no_thinking_prefix='detailed thinking off', thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='utter-project/eurollm-9b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='google/gemma-2-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-27b-it'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-v3.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=True, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}}, thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}}, base_model=None),\n",
       " Model(id='qwen/qwen2.5-coder-32b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/llama-3.3-nemotron-super-49b-v1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=True, thinking_prefix='detailed thinking on', no_thinking_prefix='detailed thinking off', thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='meta/llama-guard-4-12b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='microsoft/phi-4-multimodal-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='moonshotai/kimi-k2-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='meta/llama-4-maverick-17b-128e-instruct', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mistralai/mamba-codestral-7b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='google/gemma-3n-e4b-it', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-llama-8b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/riva-translate-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='meta/llama-3.2-90b-vision-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-90b-vision-instruct/chat/completions', aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mistralai/ministral-14b-instruct-2512', model_type='vlm', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x22b-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-0528', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemoguard-8b-content-safety', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='openai/gpt-oss-120b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='google/codegemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-7b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='meta/llama-3.2-3b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='upstage/solar-10.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-solar-10_7b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='minimaxai/minimax-m2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/llama-3.1-nemoguard-8b-topic-control', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-mini-hindi-4b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='rakuten/rakutenai-7b-chat', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='marin/marin-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-8b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-8b'], supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='moonshotai/kimi-k2-thinking', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, supports_structured_output=True, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='abacusai/dracarys-llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None),\n",
       " Model(id='deepseek-ai/deepseek-r1-distill-qwen-14b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, supports_structured_output=False, supports_thinking=False, thinking_prefix=None, no_thinking_prefix=None, thinking_param_enable=None, thinking_param_disable=None, base_model=None)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a85e964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id='qwen/qwq-32b' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=False supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='nvidia/nemotron-3-nano-30b-a3b' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=True thinking_prefix=None no_thinking_prefix=None thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}} thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}} base_model=None\n",
      "id='meta/llama-3.1-405b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='ibm/granite-3.3-8b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=False supports_thinking=True thinking_prefix=None no_thinking_prefix=None thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}} thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}} base_model=None\n",
      "id='qwen/qwen3-235b-a22b' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=False supports_thinking=True thinking_prefix=None no_thinking_prefix=None thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}} thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}} base_model=None\n",
      "id='deepseek-ai/deepseek-v3.1-terminus' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=True thinking_prefix=None no_thinking_prefix=None thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}} thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}} base_model=None\n",
      "id='qwen/qwen3-next-80b-a3b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='mistralai/mistral-small-3.1-24b-instruct-2503' model_type='vlm' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=False supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='moonshotai/kimi-k2-instruct-0905' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='mistralai/mistral-nemotron' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=False supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='microsoft/phi-4-mini-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=False supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='stepfun-ai/step-3.5-flash' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='bytedance/seed-oss-36b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='nv-mistralai/mistral-nemo-12b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='meta/llama-3.2-1b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='nvidia/nvidia-nemotron-nano-9b-v2' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=True thinking_prefix='/think' no_thinking_prefix='/no_think' thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='openai/gpt-oss-20b' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='qwen/qwen3-next-80b-a3b-thinking' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='meta/llama-3.1-8b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='meta/llama-3.3-70b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='deepseek-ai/deepseek-v3.2' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=True thinking_prefix=None no_thinking_prefix=None thinking_param_enable={'chat_template_kwargs': {'enable_thinking': True}} thinking_param_disable={'chat_template_kwargs': {'enable_thinking': False}} base_model=None\n",
      "id='nvidia/llama-3.3-nemotron-super-49b-v1.5' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=True thinking_prefix='/think' no_thinking_prefix='/no_think' thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='nvidia/llama-3.1-nemotron-nano-4b-v1.1' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=False supports_thinking=True thinking_prefix='detailed thinking on' no_thinking_prefix='detailed thinking off' thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='meta/llama-3.1-70b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='nvidia/llama-3.1-nemotron-ultra-253b-v1' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=False supports_thinking=True thinking_prefix='detailed thinking on' no_thinking_prefix='detailed thinking off' thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='nvidia/llama-3.3-nemotron-super-49b-v1' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=True thinking_prefix='detailed thinking on' no_thinking_prefix='detailed thinking off' thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='moonshotai/kimi-k2-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='deepseek-ai/deepseek-r1-0528' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=False supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='openai/gpt-oss-120b' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='meta/llama-3.2-3b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='minimaxai/minimax-m2' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n",
      "id='moonshotai/kimi-k2-thinking' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True supports_thinking=False thinking_prefix=None no_thinking_prefix=None thinking_param_enable=None thinking_param_disable=None base_model=None\n"
     ]
    }
   ],
   "source": [
    "tool_models = [model for model in ChatNVIDIA.get_available_models() if model.supports_tools]\n",
    "for elem in tool_models:\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf169b71",
   "metadata": {},
   "source": [
    "## Customising your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f50c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    model=\"gpt-5-nano\",\n",
    "    # Kwargs passed to the model:\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "response = model.invoke(\"What's the capital of the Moon?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14f450",
   "metadata": {},
   "source": [
    "## Model Providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf1d9f",
   "metadata": {},
   "source": [
    "https://docs.langchain.com/oss/python/integrations/chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db029be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(model=\"claude-sonnet-4-5\")\n",
    "\n",
    "response = model.invoke(\"What's the capital of the Moon?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
    "\n",
    "response = model.invoke(\"What's the capital of the Moon?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc723f48",
   "metadata": {},
   "source": [
    "## Initialising and invoking an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fbc17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ef4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(model=\"claude-sonnet-4-5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56a05e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5251725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the capital of the Moon?\")]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ac95763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"What's the capital of the Moon?\", additional_kwargs={}, response_metadata={}, id='27f572ca-32f5-416c-b8a0-b0f4d5c27cbf'),\n",
      "              AIMessage(content=\" The Moon does not have a capital. It's a natural satellite of Earth and lacks the necessary institutions to require a capital.\", additional_kwargs={}, response_metadata={'role': 'assistant', 'content': \" The Moon does not have a capital. It's a natural satellite of Earth and lacks the necessary institutions to require a capital.\", 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None, 'token_usage': {'prompt_tokens': 17, 'total_tokens': 45, 'completion_tokens': 28, 'prompt_tokens_details': None}, 'finish_reason': 'stop', 'model_name': 'mistralai/mixtral-8x7b-instruct-v0.1'}, id='lc_run--019c8689-cd74-79b1-a81b-39f7e9ecaef7-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 17, 'output_tokens': 28, 'total_tokens': 45}, role='assistant')]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00a8e6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Moon does not have a capital. It's a natural satellite of Earth and lacks the necessary institutions to require a capital.\n"
     ]
    }
   ],
   "source": [
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5da573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"What's the capital of the Moon?\", additional_kwargs={}, response_metadata={}, id='39aa7a57-5258-46fe-8478-24af402200fc'),\n",
      "              AIMessage(content='The capital of the Moon is Luna City.', additional_kwargs={}, response_metadata={}, id='90ef8382-7b7f-48ff-bd36-8420c13573d6', tool_calls=[], invalid_tool_calls=[]),\n",
      "              HumanMessage(content='Interesting, tell me more about Luna City', additional_kwargs={}, response_metadata={}, id='b52d9942-751a-4d85-a6cb-df47db149e24'),\n",
      "              AIMessage(content=' I apologize for the confusion, but there is actually no capital of the Moon. The Moon is not a sovereign state and does not have its own government or cities. My previous response was a playful attempt to acknowledge and add a humorous element to the fact that I could not provide a serious answer.\\n\\nLuna City is a fictional city that appears in various works of science fiction and literature. One of the most famous depictions of Luna City is in the comic strip \"Dick Tracy\" by Chester Gould, where it is introduced as a city built on the Moon in the 23rd century. The city is also mentioned in other science fiction works, such as in the \"Luna City\" series by Isaac Asimov.\\n\\nHowever, in reality, there are no cities or permanent human settlements on the Moon.', additional_kwargs={}, response_metadata={'role': 'assistant', 'content': ' I apologize for the confusion, but there is actually no capital of the Moon. The Moon is not a sovereign state and does not have its own government or cities. My previous response was a playful attempt to acknowledge and add a humorous element to the fact that I could not provide a serious answer.\\n\\nLuna City is a fictional city that appears in various works of science fiction and literature. One of the most famous depictions of Luna City is in the comic strip \"Dick Tracy\" by Chester Gould, where it is introduced as a city built on the Moon in the 23rd century. The city is also mentioned in other science fiction works, such as in the \"Luna City\" series by Isaac Asimov.\\n\\nHowever, in reality, there are no cities or permanent human settlements on the Moon.', 'refusal': None, 'annotations': None, 'audio': None, 'function_call': None, 'tool_calls': [], 'reasoning': None, 'reasoning_content': None, 'token_usage': {'prompt_tokens': 45, 'total_tokens': 224, 'completion_tokens': 179, 'prompt_tokens_details': None}, 'finish_reason': 'stop', 'model_name': 'mistralai/mixtral-8x7b-instruct-v0.1'}, id='lc_run--019c868f-8099-7170-9173-7f8d3eac9625-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 45, 'output_tokens': 179, 'total_tokens': 224}, role='assistant')]}\n"
     ]
    }
   ],
   "source": [
    "# pass chat history to the agent\n",
    "from langchain.messages import AIMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the capital of the Moon?\"),\n",
    "    AIMessage(content=\"The capital of the Moon is Luna City.\"),\n",
    "    HumanMessage(content=\"Interesting, tell me more about Luna City\")]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf635e",
   "metadata": {},
   "source": [
    "## Streaming Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, metadata in agent.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me all about Luna City, the capital of the Moon\")]},\n",
    "    stream_mode=\"messages\"\n",
    "):\n",
    "\n",
    "    # token is a message chunk with token content\n",
    "    # metadata contains which node produced the token\n",
    "    \n",
    "    if token.content:  # Check if there's actual content\n",
    "        print(token.content, end=\"\", flush=True)  # Print token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ef691a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twelve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
